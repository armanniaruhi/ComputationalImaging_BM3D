{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import .exr file"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:48:06.976613Z",
     "start_time": "2025-01-09T21:48:06.961807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fftpack import dct, idct\n",
    "from skimage import io, img_as_float\n",
    "from skimage.util import random_noise\n",
    "import imquality.brisque as brisque\n",
    "import bm3d\n",
    "from bm3d import bm3d, BM3DStages\n",
    "import bm3d.profiles as profiles\n",
    "import OpenEXR\n",
    "import Imath\n",
    "from PIL import Image, ImageDraw\n",
    "import math\n",
    "import warnings\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:48:08.332986Z",
     "start_time": "2025-01-09T21:48:08.314603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_exr(file_path):\n",
    "    \"\"\"\n",
    "    Processes an EXR file by loading its data, adjusting exposure and gamma,\n",
    "    and optionally displaying the original and adjusted images.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the EXR file.\n",
    "        ev (float): Exposure value to adjust brightness. Positive increases, negative decreases.\n",
    "        gamma (float): Gamma correction value for gamma correction.\n",
    "        plot_images (bool): If True, displays the original and adjusted images.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (noisy_img, noisy_img_adjusted), where:\n",
    "               - noisy_img is the original HDR image as a NumPy array.\n",
    "               - noisy_img_adjusted is the adjusted image.\n",
    "    \"\"\"\n",
    "    # Open the EXR file\n",
    "    exr_file = OpenEXR.InputFile(file_path)\n",
    "\n",
    "    # Get the header and dimensions\n",
    "    header = exr_file.header()\n",
    "    dw = header['dataWindow']\n",
    "    width = dw.max.x - dw.min.x + 1\n",
    "    height = dw.max.y - dw.min.y + 1\n",
    "\n",
    "    # Channels to extract\n",
    "    channels = ['R', 'G', 'B']\n",
    "\n",
    "    # Determine pixel type\n",
    "    pixel_type = Imath.PixelType(Imath.PixelType.HALF)\n",
    "\n",
    "    # Allocate memory and load channels\n",
    "    img = np.empty((height, width, len(channels)), dtype=np.float16)\n",
    "    for i, channel in enumerate(channels):\n",
    "        raw_data = exr_file.channel(channel, pixel_type)\n",
    "        img[:, :, i] = np.frombuffer(raw_data, dtype=np.float16).reshape(height, width)\n",
    "\n",
    "    # Convert to float32 for processing\n",
    "    noisy_img = img.astype(np.float32)\n",
    "\n",
    "    return noisy_img\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:48:10.518971Z",
     "start_time": "2025-01-09T21:48:10.509941Z"
    }
   },
   "source": [
    "def adjust_gamma(image, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Adjust the gamma of an image.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image.\n",
    "        gamma (float): Gamma value to apply.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Gamma-adjusted image.\n",
    "    \"\"\"\n",
    "    # Build a lookup table mapping pixel values [0, 255] to adjusted values\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    # Apply gamma correction using the lookup table\n",
    "    return cv2.LUT(image, table)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:48:12.021612Z",
     "start_time": "2025-01-09T21:48:12.007726Z"
    }
   },
   "source": [
    "def calculate_snr(image_3channel):\n",
    "    \"\"\"Calculate SNR (Signal-to-Noise Ratio) in decibels using mean and max-min methods.\"\"\"\n",
    "    # Initialize lists for SNR per channel\n",
    "    snr_mean_per_channel = []\n",
    "    snr_max_min_per_channel = []\n",
    "\n",
    "    for channel in range(image_3channel.shape[2]):\n",
    "        channel_data = image_3channel[:, :, channel]\n",
    "        \n",
    "        # Signal using max-min\n",
    "        signal_max_min = channel_data.max() - channel_data.min()\n",
    "        \n",
    "        # Signal using mean\n",
    "        signal_mean = np.mean(channel_data)\n",
    "        \n",
    "        # Noise (Standard deviation)\n",
    "        noise = np.std(channel_data)\n",
    "        \n",
    "        # SNR using max-min and mean methods\n",
    "        snr_max_min = signal_max_min / noise\n",
    "        snr_mean = signal_mean / noise\n",
    "        \n",
    "        # Append to respective lists\n",
    "        snr_max_min_per_channel.append(snr_max_min)\n",
    "        snr_mean_per_channel.append(snr_mean)\n",
    "\n",
    "    # Compute overall SNR for max-min and mean methods\n",
    "    overall_snr_max_min = np.mean(snr_max_min_per_channel)\n",
    "    overall_snr_mean = np.mean(snr_mean_per_channel)\n",
    "\n",
    "    # Convert to decibels\n",
    "    snr_max_min_db = 20 * math.log(overall_snr_max_min, 10)\n",
    "    snr_mean_db = 20 * math.log(overall_snr_mean, 10)\n",
    "\n",
    "    return snr_max_min_db, snr_mean_db\n"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:50:58.246224Z",
     "start_time": "2025-01-09T21:50:58.229120Z"
    }
   },
   "source": [
    "import torch\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "def numpy_to_torch(img_np):\n",
    "    # Ensure the dimensions are in the format [Channels, Height, Width]\n",
    "    if len(img_np.shape) == 2:  # Grayscale image\n",
    "        img_tensor = torch.tensor(img_np, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "    else:  # Color image\n",
    "        img_tensor = torch.tensor(img_np, dtype=torch.float32).permute(2, 0, 1).to(device)  # Rearrange dimensions\n",
    "\n",
    "    # Normalize the tensor values to [0, 1] (optional, depending on your use case)\n",
    "    img_tensor /= 255.0\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "    img_tensor.requires_grad = True\n",
    "    \n",
    "    return img_tensor"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries for No Reference Metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%pip install lpips"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:51:00.238673Z",
     "start_time": "2025-01-09T21:51:00.228264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from piq import BRISQUELoss, TVLoss, CLIPIQA\n",
    "import lpips"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:51:01.858403Z",
     "start_time": "2025-01-09T21:51:01.844010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU oder CPU ausw√§hlen\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA Device Name: NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:51:03.112307Z",
     "start_time": "2025-01-09T21:51:03.093470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# LPIPS-Model initialisieren (z. B. mit VGG)\n",
    "#loss_fn = lpips.LPIPS(net='vgg').to(device)  # 'alex' oder 'squeeze' sind auch Optionen\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:51:04.516195Z",
     "start_time": "2025-01-09T21:51:04.497851Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "# Warnungen ignorieren\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "def process_bm3d(\n",
    "    typ,\n",
    "    transform_2d_ht_names=[\"dct\", \"bior1.5\"],\n",
    "    max_3d_size_hts=[32],\n",
    "    max_3d_size_wieners=[32],\n",
    "    bs_wieners=[8],\n",
    "    step_wieners=[3],\n",
    "    bs_hts=[8],\n",
    "    nfs=[32],\n",
    "    search_window_hts=[39],\n",
    "    search_window_wieners=[39],\n",
    "    step_hts=[3],\n",
    "    tau_matchs=[3000],\n",
    "    tau_match_wieners=[400],\n",
    "    lambda_thr3ds=[2.7],\n",
    "    gammas=[2.0],\n",
    "    beta_wieners=[2.0]\n",
    "):\n",
    "\n",
    "    # Starte den Timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Storage for results\n",
    "    results_data = []\n",
    "    index = 0\n",
    "    y_s = [15, 16, 17, 18, 19, 20, 21, 22]\n",
    "\n",
    "    # Combine parameters for grid search\n",
    "    parameter_combinations = itertools.product(\n",
    "        y_s,\n",
    "        transform_2d_ht_names,\n",
    "        max_3d_size_hts,\n",
    "        max_3d_size_wieners,\n",
    "        bs_wieners,\n",
    "        step_wieners,\n",
    "        bs_hts,\n",
    "        nfs,\n",
    "        search_window_hts,\n",
    "        search_window_wieners,\n",
    "        step_hts,\n",
    "        tau_matchs,\n",
    "        tau_match_wieners,\n",
    "        lambda_thr3ds,\n",
    "        gammas,\n",
    "        beta_wieners,\n",
    "    )\n",
    "\n",
    "    parameter_combinations = list(parameter_combinations)  # Umwandeln in Liste\n",
    "    total_combinations = len(parameter_combinations)  # Timer\n",
    "\n",
    "\n",
    "    # Iterate through all combinations\n",
    "    for i, params in enumerate(parameter_combinations):\n",
    "        print(f\"Processing combination {i + 1}/{total_combinations}...\")  # Timer process\n",
    "        (\n",
    "            y,\n",
    "            transform_2d_ht_name,\n",
    "            max_3d_size_ht,\n",
    "            max_3d_size_wiener,\n",
    "            bs_wiener,\n",
    "            step_wiener,\n",
    "            bs_ht,\n",
    "            nf,\n",
    "            search_window_ht,\n",
    "            search_window_wiener,\n",
    "            step_ht,\n",
    "            tau_match,\n",
    "            tau_match_wiener,\n",
    "            lambda_thr3d,\n",
    "            gamma,\n",
    "            beta_wiener,\n",
    "        ) = params\n",
    "\n",
    "        sigma_psd_values = [*range(5, 31, 5)]\n",
    "        stage_args = [BM3DStages.ALL_STAGES]\n",
    "\n",
    "        # Configure BM3D profile\n",
    "        profile = profiles.BM3DProfile()\n",
    "        # Transforms used\n",
    "        profile.transform_2d_ht_name = transform_2d_ht_name  # 'bior1.5'\n",
    "        profile.transform_2d_wiener_name = \"dct\"\n",
    "\n",
    "        # -- Exact variances for correlated noise: --\n",
    "\n",
    "        # Variance calculation parameters\n",
    "        profile.nf = nf  # 32  # domain size for FFT computations\n",
    "        profile.k = 4  # how many layers of var3D to calculate\n",
    "\n",
    "        # Block matching\n",
    "        profile.gamma = gamma  # 3.0  # Block matching correction factor\n",
    "\n",
    "        # -- Classic BM3D for correlated noise --\n",
    "\n",
    "        # Hard-thresholding (HT) parameters:\n",
    "        profile.bs_ht = bs_ht  # 8  # N1 x N1 is the block size used for the hard-thresholding (HT) filtering\n",
    "        profile.step_ht = (\n",
    "            step_ht  # 3# sliding step to process every next reference block\n",
    "        )\n",
    "        profile.max_3d_size_ht = max_3d_size_ht  # 16  # maximum number of similar blocks (maximum size of the 3rd dimension of a 3D array)\n",
    "        profile.search_window_ht = search_window_ht  # 39  # side length of the search neighborhood for full-search block-matching (BM), must be odd\n",
    "        profile.tau_match = (\n",
    "            tau_match  # 3000  # threshold for the block-distance (d-distance)\n",
    "        )\n",
    "\n",
    "        # None in these parameters results in automatic parameter selection for them\n",
    "        profile.lambda_thr3d = lambda_thr3d  # None  # 2.7  # threshold parameter for the hard-thresholding in 3D transform domain\n",
    "        profile.mu2 = lambda_thr3d  # None  # 1.0\n",
    "\n",
    "        # Wiener filtering parameters:\n",
    "        profile.bs_wiener = bs_wiener  # 8\n",
    "        profile.step_wiener = step_wiener  # 3\n",
    "        profile.max_3d_size_wiener = max_3d_size_wiener  # 32\n",
    "        profile.search_window_wiener = search_window_wiener  # 39\n",
    "        profile.tau_match_wiener = tau_match_wiener  # 400\n",
    "        profile.beta_wiener = beta_wiener  # 2.0\n",
    "        profile.dec_level = (\n",
    "            0  # dec. levels of the dyadic wavelet 2D transform for blocks\n",
    "        )\n",
    "\n",
    "        output_base_folder_arm = f\"/home/arman/Documents/arman/Uni/Master/Semester 3/ip_repository/ImageProcessing/Results/Leech results/\"\n",
    "        output_base_folder_ehait = f\"C:/Users/ehait/PycharmProjects/ImageProcessing/Results/Leech results/\"\n",
    "        output_base_folder = output_base_folder_arm if os.path.exists(output_base_folder_arm) else output_base_folder_ehait\n",
    "\n",
    "        file_path_arm = \"f/home/arman/Documents/arman/Uni/Master/Semester 3/ip_repository/ImageProcessing/Dataset/leech/2024_04_25_11_54_01_img_x_15_y_{y}_r_0_g_1_b_0_cropped.exr\"\n",
    "        file_path_ehait = f\"C:/Users/ehait/PycharmProjects/ImageProcessing/Dataset/leech/2024_04_25_11_54_01_img_x_15_y_{y}_r_0_g_1_b_0_cropped.exr\"\n",
    "        file_path = file_path_arm if os.path.exists(file_path_arm.format(y=y)) else file_path_ehait.format(y=y)\n",
    "\n",
    "        # Load and preprocess noisy image\n",
    "        noisy_img = process_exr(file_path)  # 800x800\n",
    "        noisy_img = (noisy_img[:-10, :-10] * 255).astype(\"uint8\")\n",
    "\n",
    "        # Process each stage and sigma_psd value\n",
    "        for stage_arg in stage_args:\n",
    "            for sigma_psd in sigma_psd_values:\n",
    "                denoised_img = bm3d(\n",
    "                    noisy_img, sigma_psd=sigma_psd, stage_arg=stage_arg, profile=profile\n",
    "                )\n",
    "                snr_max_min_db, snr_mean_db = calculate_snr(denoised_img)\n",
    "                img_tensor = numpy_to_torch(denoised_img)\n",
    "\n",
    "                # BRISQUELoss\n",
    "                loss = BRISQUELoss(reduction=\"sum\").to(device)\n",
    "                brsique_score = loss(img_tensor)\n",
    "                brsique_score.backward()\n",
    "                brsique_score = brsique_score.item()\n",
    "\n",
    "                # TV\n",
    "                loss = TVLoss().to(device)\n",
    "                tv = loss(img_tensor)\n",
    "                tv.backward()\n",
    "                tv = tv.item()\n",
    "\n",
    "                # CLIP IQA\n",
    "                clipiqa = CLIPIQA().to(device)\n",
    "                clipiqa_score = clipiqa(img_tensor).item()#\n",
    "                \n",
    "\n",
    "                # LPIPS\n",
    "                noisy_img_torch = numpy_to_torch(noisy_img)\n",
    "                loss_fn = lpips.LPIPS(net='vgg').to(device)  # 'alex' oder 'squeeze' sind auch Optionen\n",
    "                lpips_score = loss_fn(noisy_img_torch, img_tensor).item()\n",
    "\n",
    "                denoised_img_float32 = (\n",
    "                    denoised_img.astype(\"float32\") / 255.0\n",
    "                )  # Normalize to [0, 1] range\n",
    "                # OpenEXR requires the data to be split into channels (R, G, B)\n",
    "                R = denoised_img_float32[:, :, 0].tobytes()  # Red channel\n",
    "                G = denoised_img_float32[:, :, 1].tobytes()  # Green channel\n",
    "                B = denoised_img_float32[:, :, 2].tobytes()  # Blue channel\n",
    "\n",
    "                # Define EXR header\n",
    "                header = OpenEXR.Header(\n",
    "                    denoised_img_float32.shape[1], denoised_img_float32.shape[0]\n",
    "                )  # Width, Height\n",
    "\n",
    "                # Write the data to an EXR file\n",
    "                path_final = output_base_folder + f\"{typ}-{index}_image_res.exr\"\n",
    "                exr_file = OpenEXR.OutputFile(path_final, header)\n",
    "                exr_file.writePixels({\"R\": R, \"G\": G, \"B\": B})\n",
    "                exr_file.close()\n",
    "\n",
    "                # Store all parameter values and results\n",
    "                results_data.append(\n",
    "                    {\n",
    "                        \"Coordinate y\": y,\n",
    "                        \"Sigma PSD\": sigma_psd,\n",
    "                        \"Stage_arg\": stage_arg,\n",
    "                        \"Transform 2D\": transform_2d_ht_name,\n",
    "                        \"Max 3D Size HT\": max_3d_size_ht,\n",
    "                        \"Max 3D Size Wiener\": max_3d_size_wiener,\n",
    "                        \"Block Size Wiener\": bs_wiener,\n",
    "                        \"Step Wiener\": step_wiener,\n",
    "                        \"Block Size HT\": bs_ht,\n",
    "                        \"Step HT\": step_ht,\n",
    "                        \"Number of Features (NF)\": nf,\n",
    "                        \"Search Window HT\": search_window_ht,\n",
    "                        \"Search Window Wiener\": search_window_wiener,\n",
    "                        \"Tau Match\": tau_match,\n",
    "                        \"Tau Match Wiener\": tau_match_wiener,\n",
    "                        \"Lambda Thr3D\": lambda_thr3d,\n",
    "                        \"Gamma\": gamma,\n",
    "                        \"Beta Wiener\": beta_wiener,\n",
    "                        \"SNR Max-Min (dB)\": snr_max_min_db,\n",
    "                        \"SNR Mean (dB)\": snr_mean_db,\n",
    "                        \"BRISQUE\": brsique_score,\n",
    "                        \"Total Variation\": tv,\n",
    "                        \"Clip-IQA\": clipiqa_score,\n",
    "                        \"LPIPs_score\" : lpips_score\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                excel_path_arm = f\"/home/arman/Documents/arman/Uni/Master/Semester 3/ip_repository/ImageProcessing/Results/Leech results/results_data_{{typ}}.xlsx\"\n",
    "                excel_path_ehait = f\"C:/Users/ehait/PycharmProjects/ImageProcessing/Results/Leech results/results_data_{{typ}}.xlsx\"\n",
    "                excel_path = excel_path_arm.format(typ=typ) if os.path.exists(output_base_folder_arm) else excel_path_ehait.format(typ=typ)\n",
    "\n",
    "                # Convert to a DataFrame\n",
    "                df = pd.DataFrame(results_data)\n",
    "                # Save to Excel\n",
    "                df.to_excel(excel_path, index=True)\n",
    "                index = index + 1\n",
    "\n",
    "    # Gesamtlaufzeit berechnen\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Process completed in {elapsed_time:.2f} seconds.\")"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of N patches in block"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T04:34:46.082994Z",
     "start_time": "2025-01-09T04:07:02.827089Z"
    }
   },
   "source": [
    "max_3d_size_hts=[16,32]\n",
    "max_3d_size_wieners=[16,32]\n",
    "process_bm3d(typ=\"Block_Size\", max_3d_size_hts=max_3d_size_hts, max_3d_size_wieners=max_3d_size_wieners)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing combination 1/64...\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Processing combination 2/64...\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Processing combination 3/64...\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Processing combination 4/64...\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Processing combination 5/64...\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Processing combination 6/64...\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Processing combination 7/64...\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Processing combination 8/64...\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Processing combination 9/64...\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Processing combination 10/64...\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Processing combination 11/64...\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Processing combination 12/64...\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Processing combination 13/64...\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Processing combination 14/64...\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Processing combination 15/64...\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Processing combination 16/64...\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Processing combination 17/64...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected values to be greater or equal to 0, got -0.020198315382003784",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m max_3d_size_hts\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m16\u001B[39m,\u001B[38;5;241m32\u001B[39m]\n\u001B[0;32m      2\u001B[0m max_3d_size_wieners\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m16\u001B[39m,\u001B[38;5;241m32\u001B[39m]\n\u001B[1;32m----> 3\u001B[0m \u001B[43mprocess_bm3d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtyp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mBlock_Size\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_3d_size_hts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_3d_size_hts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_3d_size_wieners\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_3d_size_wieners\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[14], line 158\u001B[0m, in \u001B[0;36mprocess_bm3d\u001B[1;34m(typ, transform_2d_ht_names, max_3d_size_hts, max_3d_size_wieners, bs_wieners, step_wieners, bs_hts, nfs, search_window_hts, search_window_wieners, step_hts, tau_matchs, tau_match_wieners, lambda_thr3ds, gammas, beta_wieners)\u001B[0m\n\u001B[0;32m    156\u001B[0m \u001B[38;5;66;03m# BRISQUELoss\u001B[39;00m\n\u001B[0;32m    157\u001B[0m loss \u001B[38;5;241m=\u001B[39m BRISQUELoss(reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msum\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m--> 158\u001B[0m brsique_score \u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    159\u001B[0m brsique_score\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m    160\u001B[0m brsique_score \u001B[38;5;241m=\u001B[39m brsique_score\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\piq\\brisque.py:120\u001B[0m, in \u001B[0;36mBRISQUELoss.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m    112\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Computation of BRISQUE score as a loss function.\u001B[39;00m\n\u001B[0;32m    113\u001B[0m \n\u001B[0;32m    114\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;124;03m        Value of BRISQUE loss to be minimized.\u001B[39;00m\n\u001B[0;32m    119\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 120\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbrisque\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkernel_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkernel_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    121\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mkernel_sigma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkernel_sigma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_range\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_range\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\piq\\brisque.py:51\u001B[0m, in \u001B[0;36mbrisque\u001B[1;34m(x, kernel_size, kernel_sigma, data_range, reduction)\u001B[0m\n\u001B[0;32m     45\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBRISQUE does not support back propagation due to bug in torch=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtorch\u001B[38;5;241m.\u001B[39m__version__\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     46\u001B[0m                   \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUpdate torch to the latest version to access full functionality of the BRIQSUE.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     47\u001B[0m                   \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMore info is available at https://github.com/photosynthesis-team/piq/pull/79 and\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     48\u001B[0m                   \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://github.com/pytorch/pytorch/issues/38869.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m kernel_size \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mKernel size must be odd, got [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkernel_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m---> 51\u001B[0m \u001B[43m_validate_input\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim_range\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_range\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_range\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     53\u001B[0m x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mfloat\u001B[39m(data_range) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m255\u001B[39m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m:\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;66;03m# Mimic matlab rgb2gray, which keeps image in uint8 during colour conversion\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\piq\\utils\\common.py:100\u001B[0m, in \u001B[0;36m_validate_input\u001B[1;34m(tensors, dim_range, data_range, size_range, check_for_channels_first)\u001B[0m\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m dim_range[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m dim_range[\u001B[38;5;241m1\u001B[39m], \\\n\u001B[0;32m     97\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpected number of dimensions to be between \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdim_range[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdim_range[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;241m.\u001B[39mdim()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m data_range[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m<\u001B[39m data_range[\u001B[38;5;241m1\u001B[39m]:\n\u001B[1;32m--> 100\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m data_range[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m t\u001B[38;5;241m.\u001B[39mmin(), \\\n\u001B[0;32m    101\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpected values to be greater or equal to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_range[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;241m.\u001B[39mmin()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    102\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m t\u001B[38;5;241m.\u001B[39mmax() \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m data_range[\u001B[38;5;241m1\u001B[39m], \\\n\u001B[0;32m    103\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpected values to be lower or equal to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_range[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;241m.\u001B[39mmax()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m check_for_channels_first:\n",
      "\u001B[1;31mAssertionError\u001B[0m: Expected values to be greater or equal to 0, got -0.020198315382003784"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of Patch size hard and Patch size wien"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:51:46.274329Z",
     "start_time": "2025-01-09T21:51:08.302737Z"
    }
   },
   "source": [
    "bs_wieners = [8,16]\n",
    "bs_hts = [8,16]\n",
    "process_bm3d(typ=\"Patch_Size\", bs_hts=bs_hts, bs_wieners=bs_wieners)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing combination 1/64...\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\ehait\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m bs_wieners \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m8\u001B[39m,\u001B[38;5;241m16\u001B[39m]\n\u001B[0;32m      2\u001B[0m bs_hts \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m8\u001B[39m,\u001B[38;5;241m16\u001B[39m]\n\u001B[1;32m----> 3\u001B[0m \u001B[43mprocess_bm3d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtyp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPatch_Size\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbs_hts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbs_hts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbs_wieners\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbs_wieners\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[28], line 142\u001B[0m, in \u001B[0;36mprocess_bm3d\u001B[1;34m(typ, transform_2d_ht_names, max_3d_size_hts, max_3d_size_wieners, bs_wieners, step_wieners, bs_hts, nfs, search_window_hts, search_window_wieners, step_hts, tau_matchs, tau_match_wieners, lambda_thr3ds, gammas, beta_wieners)\u001B[0m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m stage_arg \u001B[38;5;129;01min\u001B[39;00m stage_args:\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m sigma_psd \u001B[38;5;129;01min\u001B[39;00m sigma_psd_values:\n\u001B[1;32m--> 142\u001B[0m         denoised_img \u001B[38;5;241m=\u001B[39m \u001B[43mbm3d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    143\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnoisy_img\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msigma_psd\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msigma_psd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstage_arg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstage_arg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprofile\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprofile\u001B[49m\n\u001B[0;32m    144\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    145\u001B[0m         snr_max_min_db, snr_mean_db \u001B[38;5;241m=\u001B[39m calculate_snr(denoised_img)\n\u001B[0;32m    146\u001B[0m         img_tensor \u001B[38;5;241m=\u001B[39m numpy_to_torch(denoised_img)\n",
      "File \u001B[1;32m~\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\bm3d\\__init__.py:218\u001B[0m, in \u001B[0;36mbm3d\u001B[1;34m(z, sigma_psd, profile, stage_arg, blockmatches)\u001B[0m\n\u001B[0;32m    216\u001B[0m         sigma_psd \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([[sigma_psd]])\u001B[38;5;241m.\u001B[39mtranspose((\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m    217\u001B[0m     z \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([z])\u001B[38;5;241m.\u001B[39mtranspose((\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m))\n\u001B[1;32m--> 218\u001B[0m     y_hat \u001B[38;5;241m=\u001B[39m \u001B[43m_bm4d\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbm4d_multichannel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msigma_psd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconverted_profile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstage_arg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    219\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    220\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m incl_blockmatches:\n",
      "File \u001B[1;32m~\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\bm4d\\__init__.py:80\u001B[0m, in \u001B[0;36mbm4d_multichannel\u001B[1;34m(z, sigma_psd, profile, stage_arg)\u001B[0m\n\u001B[0;32m     76\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39msqueeze(sigma_psd)\n\u001B[0;32m     77\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m sigma_psd[index]\n\u001B[1;32m---> 80\u001B[0m denoi, match_arrs \u001B[38;5;241m=\u001B[39m \u001B[43mbm4d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mz\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mget_sigma\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprofile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstage_arg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     81\u001B[0m full_denoi\u001B[38;5;241m.\u001B[39mappend(denoi)\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mlen\u001B[39m(z)):\n",
      "File \u001B[1;32m~\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\bm4d\\__init__.py:217\u001B[0m, in \u001B[0;36mbm4d\u001B[1;34m(z, sigma_psd, profile, stage_arg, blockmatches)\u001B[0m\n\u001B[0;32m    213\u001B[0m t_forward, t_inverse, hadper_trans_single_den, \\\n\u001B[0;32m    214\u001B[0m     inverse_hadper_trans_single_den, wwin3d \u001B[38;5;241m=\u001B[39m _get_transforms(pro, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    216\u001B[0m \u001B[38;5;66;03m# Call the actual hard-thresholding step with the acquired parameters\u001B[39;00m\n\u001B[1;32m--> 217\u001B[0m y_hat, bm_out_ht \u001B[38;5;241m=\u001B[39m \u001B[43mht_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpsd_blur\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpro\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt_forward\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt_inverse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mqshifts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhadper_trans_single_den\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    218\u001B[0m \u001B[43m                        \u001B[49m\u001B[43minverse_hadper_trans_single_den\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwwin3d\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mblockmatches\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbm_in_ht\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    220\u001B[0m \u001B[38;5;66;03m# Residual denoising, HT\u001B[39;00m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pro\u001B[38;5;241m.\u001B[39mdenoise_residual:\n",
      "File \u001B[1;32m~\\PycharmProjects\\renderingTonemapping\\.venv\\lib\\site-packages\\bm4d\\bm4d_ctypes.py:609\u001B[0m, in \u001B[0;36mbm4d_ht\u001B[1;34m(z, psd, pro, t_forward, t_inverse, qshifts, hadper_trans_single_den, inverse_hadper_trans_single_den, wwin3d, refilter, blockmatches)\u001B[0m\n\u001B[0;32m    606\u001B[0m c_psd \u001B[38;5;241m=\u001B[39m conv_to_array(np\u001B[38;5;241m.\u001B[39mascontiguousarray(psd\u001B[38;5;241m.\u001B[39mflatten(), dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32), ctype\u001B[38;5;241m=\u001B[39mctypes\u001B[38;5;241m.\u001B[39mc_float)\n\u001B[0;32m    607\u001B[0m c_est \u001B[38;5;241m=\u001B[39m conv_to_array(np\u001B[38;5;241m.\u001B[39mascontiguousarray(res\u001B[38;5;241m.\u001B[39mflatten(), dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32), ctype\u001B[38;5;241m=\u001B[39mctypes\u001B[38;5;241m.\u001B[39mc_float)\n\u001B[1;32m--> 609\u001B[0m \u001B[43mfunc_ht\u001B[49m\u001B[43m(\u001B[49m\u001B[43mc_z\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc_psd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransforms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc_est\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    610\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbyref\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmatchtables\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mmatchtables\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mmatchtables\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    612\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(z_shape[\u001B[38;5;241m0\u001B[39m]):\n\u001B[0;32m    613\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(z_shape[\u001B[38;5;241m1\u001B[39m]):\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Variable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
