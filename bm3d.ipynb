{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T16:28:43.792389Z",
     "start_time": "2024-11-19T16:28:43.787249Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fftpack import dct, idct\n",
    "from skimage import io, img_as_float\n",
    "from skimage.util import random_noise\n",
    "import imquality.brisque as brisque\n",
    "import bm3d\n",
    "from bm3d import bm3d, BM3DStages\n",
    "import bm3d.profiles as profiles\n",
    "import OpenEXR\n",
    "import Imath\n",
    "from PIL import Image, ImageDraw\n",
    "import math  \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import .exr file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_exr(file_path):\n",
    "    \"\"\"\n",
    "    Processes an EXR file by loading its data, adjusting exposure and gamma, \n",
    "    and optionally displaying the original and adjusted images.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the EXR file.\n",
    "        ev (float): Exposure value to adjust brightness. Positive increases, negative decreases.\n",
    "        gamma (float): Gamma correction value for gamma correction.\n",
    "        plot_images (bool): If True, displays the original and adjusted images.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (noisy_img, noisy_img_adjusted), where:\n",
    "               - noisy_img is the original HDR image as a NumPy array.\n",
    "               - noisy_img_adjusted is the adjusted image.\n",
    "    \"\"\"\n",
    "    # Open the EXR file\n",
    "    exr_file = OpenEXR.InputFile(file_path)\n",
    "\n",
    "    # Get the header and dimensions\n",
    "    header = exr_file.header()\n",
    "    dw = header['dataWindow']\n",
    "    width = dw.max.x - dw.min.x + 1\n",
    "    height = dw.max.y - dw.min.y + 1\n",
    "\n",
    "    # Channels to extract\n",
    "    channels = ['R', 'G', 'B']\n",
    "\n",
    "    # Determine pixel type\n",
    "    pixel_type = Imath.PixelType(Imath.PixelType.HALF)\n",
    "\n",
    "    # Allocate memory and load channels\n",
    "    img = np.empty((height, width, len(channels)), dtype=np.float16)\n",
    "    for i, channel in enumerate(channels):\n",
    "        raw_data = exr_file.channel(channel, pixel_type)\n",
    "        img[:, :, i] = np.frombuffer(raw_data, dtype=np.float16).reshape(height, width)\n",
    "\n",
    "    # Convert to float32 for processing\n",
    "    noisy_img = img.astype(np.float32)\n",
    "\n",
    "    return noisy_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_gamma(image, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Adjust the gamma of an image.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image.\n",
    "        gamma (float): Gamma value to apply.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Gamma-adjusted image.\n",
    "    \"\"\"\n",
    "    # Build a lookup table mapping pixel values [0, 255] to adjusted values\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    # Apply gamma correction using the lookup table\n",
    "    return cv2.LUT(image, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_snr(image_3channel):\n",
    "    \"\"\"Calculate SNR (Signal-to-Noise Ratio) in decibels using mean and max-min methods.\"\"\"\n",
    "    # Initialize lists for SNR per channel\n",
    "    snr_mean_per_channel = []\n",
    "    snr_max_min_per_channel = []\n",
    "\n",
    "    for channel in range(image_3channel.shape[2]):\n",
    "        channel_data = image_3channel[:, :, channel]\n",
    "        \n",
    "        # Signal using max-min\n",
    "        signal_max_min = channel_data.max() - channel_data.min()\n",
    "        \n",
    "        # Signal using mean\n",
    "        signal_mean = np.mean(channel_data)\n",
    "        \n",
    "        # Noise (Standard deviation)\n",
    "        noise = np.std(channel_data)\n",
    "        \n",
    "        # SNR using max-min and mean methods\n",
    "        snr_max_min = signal_max_min / noise\n",
    "        snr_mean = signal_mean / noise\n",
    "        \n",
    "        # Append to respective lists\n",
    "        snr_max_min_per_channel.append(snr_max_min)\n",
    "        snr_mean_per_channel.append(snr_mean)\n",
    "\n",
    "    # Compute overall SNR for max-min and mean methods\n",
    "    overall_snr_max_min = np.mean(snr_max_min_per_channel)\n",
    "    overall_snr_mean = np.mean(snr_mean_per_channel)\n",
    "\n",
    "    # Convert to decibels\n",
    "    snr_max_min_db = 20 * math.log(overall_snr_max_min, 10)\n",
    "    snr_mean_db = 20 * math.log(overall_snr_mean, 10)\n",
    "\n",
    "    return snr_max_min_db, snr_mean_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "def numpy_to_torch(img_np):\n",
    "    # Ensure the dimensions are in the format [Channels, Height, Width]\n",
    "    if len(img_np.shape) == 2:  # Grayscale image\n",
    "        img_tensor = torch.tensor(img_np, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "    else:  # Color image\n",
    "        img_tensor = torch.tensor(img_np, dtype=torch.float32).permute(2, 0, 1)  # Rearrange dimensions\n",
    "\n",
    "    # Normalize the tensor values to [0, 1] (optional, depending on your use case)\n",
    "    img_tensor /= 255.0\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "    img_tensor.requires_grad = True\n",
    "    \n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries for No Reference Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599.96s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "Requirement already satisfied: lpips in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (0.1.4)\n",
      "Requirement already satisfied: torch>=0.4.0 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from lpips) (2.4.1)\n",
      "Requirement already satisfied: torchvision>=0.2.1 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from lpips) (0.19.1)\n",
      "Requirement already satisfied: numpy>=1.14.3 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from lpips) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from lpips) (1.14.1)\n",
      "Requirement already satisfied: tqdm>=4.28.1 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from lpips) (4.66.5)\n",
      "Requirement already satisfied: filelock in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from torch>=0.4.0->lpips) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from torch>=0.4.0->lpips) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from torch>=0.4.0->lpips) (1.13.2)\n",
      "Requirement already satisfied: networkx in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from torch>=0.4.0->lpips) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from torch>=0.4.0->lpips) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from torch>=0.4.0->lpips) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from torch>=0.4.0->lpips) (75.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from torch>=0.4.0->lpips) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from torch>=0.4.0->lpips) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from torch>=0.4.0->lpips) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from torch>=0.4.0->lpips) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from torch>=0.4.0->lpips) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from torch>=0.4.0->lpips) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from torch>=0.4.0->lpips) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from torch>=0.4.0->lpips) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from torch>=0.4.0->lpips) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from torch>=0.4.0->lpips) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from torch>=0.4.0->lpips) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from torch>=0.4.0->lpips) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=0.4.0->lpips) (12.6.68)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from torchvision>=0.2.1->lpips) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from jinja2->torch>=0.4.0->lpips) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/arman/.conda/envs/DL/lib/python3.12/site-packages (from sympy->torch>=0.4.0->lpips) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from piq import BRISQUELoss, TVLoss, CLIPIQA\n",
    "import lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arman/.conda/envs/DL/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/arman/.conda/envs/DL/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/arman/.conda/envs/DL/lib/python3.12/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arman/.conda/envs/DL/lib/python3.12/site-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    }
   ],
   "source": [
    "# LPIPS-Model initialisieren (z. B. mit VGG)\n",
    "loss_fn = lpips.LPIPS(net='vgg')  # 'alex' oder 'squeeze' sind auch Optionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "\n",
    "def process_bm3d(\n",
    "    typ,\n",
    "    transform_2d_ht_names=[\"dct\", \"bior1.5\"],\n",
    "    max_3d_size_hts=[32],\n",
    "    max_3d_size_wieners=[32],\n",
    "    bs_wieners=[8],\n",
    "    step_wieners=[3],\n",
    "    bs_hts=[8],\n",
    "    nfs=[32],\n",
    "    search_window_hts=[39],\n",
    "    search_window_wieners=[39],\n",
    "    step_hts=[3],\n",
    "    tau_matchs=[3000],\n",
    "    tau_match_wieners=[400],\n",
    "    lambda_thr3ds=[2.7],\n",
    "    gammas=[2.0],\n",
    "    beta_wieners=[2.0]\n",
    "):\n",
    "\n",
    "    # Storage for results\n",
    "    results_data = []\n",
    "    index = 0\n",
    "    y_s = [15, 16, 17, 18, 19, 20, 21, 22]\n",
    "\n",
    "    # Combine parameters for grid search\n",
    "    parameter_combinations = itertools.product(\n",
    "        y_s,\n",
    "        transform_2d_ht_names,\n",
    "        max_3d_size_hts,\n",
    "        max_3d_size_wieners,\n",
    "        bs_wieners,\n",
    "        step_wieners,\n",
    "        bs_hts,\n",
    "        nfs,\n",
    "        search_window_hts,\n",
    "        search_window_wieners,\n",
    "        step_hts,\n",
    "        tau_matchs,\n",
    "        tau_match_wieners,\n",
    "        lambda_thr3ds,\n",
    "        gammas,\n",
    "        beta_wieners,\n",
    "    )\n",
    "\n",
    "    # Iterate through all combinations\n",
    "    for params in parameter_combinations:\n",
    "        (\n",
    "            y,\n",
    "            transform_2d_ht_name,\n",
    "            max_3d_size_ht,\n",
    "            max_3d_size_wiener,\n",
    "            bs_wiener,\n",
    "            step_wiener,\n",
    "            bs_ht,\n",
    "            nf,\n",
    "            search_window_ht,\n",
    "            search_window_wiener,\n",
    "            step_ht,\n",
    "            tau_match,\n",
    "            tau_match_wiener,\n",
    "            lambda_thr3d,\n",
    "            gamma,\n",
    "            beta_wiener,\n",
    "        ) = params\n",
    "\n",
    "        sigma_psd_values = [*range(5, 31, 5)]\n",
    "        stage_args = [BM3DStages.ALL_STAGES]\n",
    "\n",
    "        # Configure BM3D profile\n",
    "        profile = profiles.BM3DProfile()\n",
    "        # Transforms used\n",
    "        profile.transform_2d_ht_name = transform_2d_ht_name  # 'bior1.5'\n",
    "        profile.transform_2d_wiener_name = \"dct\"\n",
    "\n",
    "        # -- Exact variances for correlated noise: --\n",
    "\n",
    "        # Variance calculation parameters\n",
    "        profile.nf = nf  # 32  # domain size for FFT computations\n",
    "        profile.k = 4  # how many layers of var3D to calculate\n",
    "\n",
    "        # Block matching\n",
    "        profile.gamma = gamma  # 3.0  # Block matching correction factor\n",
    "\n",
    "        # -- Classic BM3D for correlated noise --\n",
    "\n",
    "        # Hard-thresholding (HT) parameters:\n",
    "        profile.bs_ht = bs_ht  # 8  # N1 x N1 is the block size used for the hard-thresholding (HT) filtering\n",
    "        profile.step_ht = (\n",
    "            step_ht  # 3# sliding step to process every next reference block\n",
    "        )\n",
    "        profile.max_3d_size_ht = max_3d_size_ht  # 16  # maximum number of similar blocks (maximum size of the 3rd dimension of a 3D array)\n",
    "        profile.search_window_ht = search_window_ht  # 39  # side length of the search neighborhood for full-search block-matching (BM), must be odd\n",
    "        profile.tau_match = (\n",
    "            tau_match  # 3000  # threshold for the block-distance (d-distance)\n",
    "        )\n",
    "\n",
    "        # None in these parameters results in automatic parameter selection for them\n",
    "        profile.lambda_thr3d = lambda_thr3d  # None  # 2.7  # threshold parameter for the hard-thresholding in 3D transform domain\n",
    "        profile.mu2 = lambda_thr3d  # None  # 1.0\n",
    "\n",
    "        # Wiener filtering parameters:\n",
    "        profile.bs_wiener = bs_wiener  # 8\n",
    "        profile.step_wiener = step_wiener  # 3\n",
    "        profile.max_3d_size_wiener = max_3d_size_wiener  # 32\n",
    "        profile.search_window_wiener = search_window_wiener  # 39\n",
    "        profile.tau_match_wiener = tau_match_wiener  # 400\n",
    "        profile.beta_wiener = beta_wiener  # 2.0\n",
    "        profile.dec_level = (\n",
    "            0  # dec. levels of the dyadic wavelet 2D transform for blocks\n",
    "        )\n",
    "\n",
    "        output_base_folder = f\"/home/arman/Documents/arman/Uni/Master/Semester 3/ip_repository/ImageProcessing/Results/Leech results/\"\n",
    "        file_path = f\"/home/arman/Documents/arman/Uni/Master/Semester 3/ip_repository/ImageProcessing/Dataset/leech/2024_04_25_11_54_01_img_x_15_y_{y}_r_0_g_1_b_0_cropped.exr\"\n",
    "\n",
    "        # Load and preprocess noisy image\n",
    "        noisy_img = process_exr(file_path)  # 800x800\n",
    "        noisy_img = (noisy_img[:-10, :-10] * 255).astype(\"uint8\")\n",
    "\n",
    "        # Process each stage and sigma_psd value\n",
    "        for stage_arg in stage_args:\n",
    "            for sigma_psd in sigma_psd_values:\n",
    "                denoised_img = bm3d(\n",
    "                    noisy_img, sigma_psd=sigma_psd, stage_arg=stage_arg, profile=profile\n",
    "                )\n",
    "                snr_max_min_db, snr_mean_db = calculate_snr(denoised_img)\n",
    "                img_tensor = numpy_to_torch(denoised_img)\n",
    "\n",
    "                # BRISQUELoss\n",
    "                loss = BRISQUELoss(reduction=\"sum\")\n",
    "                brsique_score = loss(img_tensor)\n",
    "                brsique_score.backward()\n",
    "                brsique_score = brsique_score.item()\n",
    "\n",
    "                # TV\n",
    "                loss = TVLoss()\n",
    "                tv = loss(img_tensor)\n",
    "                tv.backward()\n",
    "                tv = tv.item()\n",
    "\n",
    "                # CLIP IQA\n",
    "                clipiqa = CLIPIQA()\n",
    "                clipiqa_score = clipiqa(img_tensor).item()#\n",
    "                \n",
    "\n",
    "                # LPIPS\n",
    "                noisy_img_torch = numpy_to_torch(noisy_img)\n",
    "                lpips_score = loss_fn(noisy_img_torch, img_tensor).item()\n",
    "\n",
    "                denoised_img_float32 = (\n",
    "                    denoised_img.astype(\"float32\") / 255.0\n",
    "                )  # Normalize to [0, 1] range\n",
    "                # OpenEXR requires the data to be split into channels (R, G, B)\n",
    "                R = denoised_img_float32[:, :, 0].tobytes()  # Red channel\n",
    "                G = denoised_img_float32[:, :, 1].tobytes()  # Green channel\n",
    "                B = denoised_img_float32[:, :, 2].tobytes()  # Blue channel\n",
    "\n",
    "                # Define EXR header\n",
    "                header = OpenEXR.Header(\n",
    "                    denoised_img_float32.shape[1], denoised_img_float32.shape[0]\n",
    "                )  # Width, Height\n",
    "\n",
    "                # Write the data to an EXR file\n",
    "                path_final = output_base_folder + f\"{typ}-{index}_image_res.exr\"\n",
    "                exr_file = OpenEXR.OutputFile(path_final, header)\n",
    "                exr_file.writePixels({\"R\": R, \"G\": G, \"B\": B})\n",
    "                exr_file.close()\n",
    "\n",
    "                # Store all parameter values and results\n",
    "                results_data.append(\n",
    "                    {\n",
    "                        \"Coordinate y\": y,\n",
    "                        \"Sigma PSD\": sigma_psd,\n",
    "                        \"Stage_arg\": stage_arg,\n",
    "                        \"Transform 2D\": transform_2d_ht_name,\n",
    "                        \"Max 3D Size HT\": max_3d_size_ht,\n",
    "                        \"Max 3D Size Wiener\": max_3d_size_wiener,\n",
    "                        \"Block Size Wiener\": bs_wiener,\n",
    "                        \"Step Wiener\": step_wiener,\n",
    "                        \"Block Size HT\": bs_ht,\n",
    "                        \"Step HT\": step_ht,\n",
    "                        \"Number of Features (NF)\": nf,\n",
    "                        \"Search Window HT\": search_window_ht,\n",
    "                        \"Search Window Wiener\": search_window_wiener,\n",
    "                        \"Tau Match\": tau_match,\n",
    "                        \"Tau Match Wiener\": tau_match_wiener,\n",
    "                        \"Lambda Thr3D\": lambda_thr3d,\n",
    "                        \"Gamma\": gamma,\n",
    "                        \"Beta Wiener\": beta_wiener,\n",
    "                        \"SNR Max-Min (dB)\": snr_max_min_db,\n",
    "                        \"SNR Mean (dB)\": snr_mean_db,\n",
    "                        \"BRISQUE\": brsique_score,\n",
    "                        \"Total Variation\": tv,\n",
    "                        \"Clip-IQA\": clipiqa_score,\n",
    "                        \"LPIPs_score\" : lpips_score\n",
    "                    }\n",
    "                )\n",
    "                # Convert to a DataFrame\n",
    "                df = pd.DataFrame(results_data)\n",
    "                # Save to Excel\n",
    "                df.to_excel(\n",
    "                    f\"/home/arman/Documents/arman/Uni/Master/Semester 3/ip_repository/ImageProcessing/Results/Leech results/results_data_{typ}.xlsx\",\n",
    "                    index=True,\n",
    "                )\n",
    "                index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of N patches in block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m max_3d_size_hts\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m32\u001b[39m]\n\u001b[1;32m      2\u001b[0m max_3d_size_wieners\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m32\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m \u001b[43mprocess_bm3d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBlock_Size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_3d_size_hts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_3d_size_hts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_3d_size_wieners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_3d_size_wieners\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 126\u001b[0m, in \u001b[0;36mprocess_bm3d\u001b[0;34m(typ, transform_2d_ht_names, max_3d_size_hts, max_3d_size_wieners, bs_wieners, step_wieners, bs_hts, nfs, search_window_hts, search_window_wieners, step_hts, tau_matchs, tau_match_wieners, lambda_thr3ds, gammas, beta_wieners)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stage_arg \u001b[38;5;129;01min\u001b[39;00m stage_args:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sigma_psd \u001b[38;5;129;01min\u001b[39;00m sigma_psd_values:\n\u001b[0;32m--> 126\u001b[0m         denoised_img \u001b[38;5;241m=\u001b[39m \u001b[43mbm3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnoisy_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_psd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma_psd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage_arg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstage_arg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m         snr_max_min_db, snr_mean_db \u001b[38;5;241m=\u001b[39m calculate_snr(denoised_img)\n\u001b[1;32m    130\u001b[0m         img_tensor \u001b[38;5;241m=\u001b[39m numpy_to_torch(denoised_img)\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.12/site-packages/bm3d/__init__.py:218\u001b[0m, in \u001b[0;36mbm3d\u001b[0;34m(z, sigma_psd, profile, stage_arg, blockmatches)\u001b[0m\n\u001b[1;32m    216\u001b[0m         sigma_psd \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[sigma_psd]])\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    217\u001b[0m     z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([z])\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m--> 218\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[43m_bm4d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm4d_multichannel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_psd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverted_profile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage_arg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incl_blockmatches:\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.12/site-packages/bm4d/__init__.py:84\u001b[0m, in \u001b[0;36mbm4d_multichannel\u001b[0;34m(z, sigma_psd, profile, stage_arg)\u001b[0m\n\u001b[1;32m     81\u001b[0m full_denoi\u001b[38;5;241m.\u001b[39mappend(denoi)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(z)):\n\u001b[0;32m---> 84\u001b[0m     denoi \u001b[38;5;241m=\u001b[39m \u001b[43mbm4d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_sigma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage_arg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch_arrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     full_denoi\u001b[38;5;241m.\u001b[39mappend(denoi)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(z, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.12/site-packages/bm4d/__init__.py:255\u001b[0m, in \u001b[0;36mbm4d\u001b[0;34m(z, sigma_psd, profile, stage_arg, blockmatches)\u001b[0m\n\u001b[1;32m    251\u001b[0m t_forward, t_inverse, hadper_trans_single_den, \\\n\u001b[1;32m    252\u001b[0m     inverse_hadper_trans_single_den, wwin3d \u001b[38;5;241m=\u001b[39m _get_transforms(pro, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# Wiener filtering\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m y_hat, bm_out_wie \u001b[38;5;241m=\u001b[39m \u001b[43mwie_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpsd_blur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpro\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqshifts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhadper_trans_single_den\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m                            \u001b[49m\u001b[43minverse_hadper_trans_single_den\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwwin3d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblockmatches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbm_in_wie\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Residual denoising, Wiener\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pro\u001b[38;5;241m.\u001b[39mdenoise_residual:\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.12/site-packages/bm4d/bm4d_ctypes.py:727\u001b[0m, in \u001b[0;36mbm4d_wie\u001b[0;34m(z, psd, pro, t_forward, t_inverse, qshifts, hadper_trans_single_den, inverse_hadper_trans_single_den, wwin3d, ref, refilter, blockmatches)\u001b[0m\n\u001b[1;32m    723\u001b[0m ref \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(ref, [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    725\u001b[0m res \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(z_shape)\n\u001b[0;32m--> 727\u001b[0m c_z \u001b[38;5;241m=\u001b[39m \u001b[43mconv_to_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_float\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m c_psd \u001b[38;5;241m=\u001b[39m conv_to_array(np\u001b[38;5;241m.\u001b[39mascontiguousarray(psd\u001b[38;5;241m.\u001b[39mflatten(), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), ctype\u001b[38;5;241m=\u001b[39mctypes\u001b[38;5;241m.\u001b[39mc_float)\n\u001b[1;32m    729\u001b[0m c_est \u001b[38;5;241m=\u001b[39m conv_to_array(np\u001b[38;5;241m.\u001b[39mascontiguousarray(res\u001b[38;5;241m.\u001b[39mflatten(), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), ctype\u001b[38;5;241m=\u001b[39mctypes\u001b[38;5;241m.\u001b[39mc_float)\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.12/site-packages/bm4d/bm4d_ctypes.py:282\u001b[0m, in \u001b[0;36mconv_to_array\u001b[0;34m(pyarr, ctype)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconv_to_array\u001b[39m(pyarr, ctype\u001b[38;5;241m=\u001b[39mctypes\u001b[38;5;241m.\u001b[39mc_float):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    Convert Python array to C array\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03m    :param pyarr: python array\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m    :param ctype: type of resulting element\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    :return: C array (pointer)\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mctype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpyarr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpyarr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_3d_size_hts=[16,32]\n",
    "max_3d_size_wieners=[16,32]\n",
    "process_bm3d(typ=\"Block_Size\", max_3d_size_hts=max_3d_size_hts, max_3d_size_wieners=max_3d_size_wieners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of Patch size hard and Patch size wien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arman/.conda/envs/DL/lib/python3.12/site-packages/piq/utils/common.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(download_target, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m bs_wieners \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m16\u001b[39m]\n\u001b[1;32m      2\u001b[0m bs_hts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m16\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m \u001b[43mprocess_bm3d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs_hts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbs_hts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs_wieners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbs_wieners\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 126\u001b[0m, in \u001b[0;36mprocess_bm3d\u001b[0;34m(typ, transform_2d_ht_names, max_3d_size_hts, max_3d_size_wieners, bs_wieners, step_wieners, bs_hts, nfs, search_window_hts, search_window_wieners, step_hts, tau_matchs, tau_match_wieners, lambda_thr3ds, gammas, beta_wieners)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stage_arg \u001b[38;5;129;01min\u001b[39;00m stage_args:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sigma_psd \u001b[38;5;129;01min\u001b[39;00m sigma_psd_values:\n\u001b[0;32m--> 126\u001b[0m         denoised_img \u001b[38;5;241m=\u001b[39m \u001b[43mbm3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnoisy_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_psd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma_psd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage_arg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstage_arg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m         snr_max_min_db, snr_mean_db \u001b[38;5;241m=\u001b[39m calculate_snr(denoised_img)\n\u001b[1;32m    130\u001b[0m         img_tensor \u001b[38;5;241m=\u001b[39m numpy_to_torch(denoised_img)\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.12/site-packages/bm3d/__init__.py:218\u001b[0m, in \u001b[0;36mbm3d\u001b[0;34m(z, sigma_psd, profile, stage_arg, blockmatches)\u001b[0m\n\u001b[1;32m    216\u001b[0m         sigma_psd \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[sigma_psd]])\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    217\u001b[0m     z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([z])\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m--> 218\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[43m_bm4d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm4d_multichannel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_psd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverted_profile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage_arg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incl_blockmatches:\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.12/site-packages/bm4d/__init__.py:84\u001b[0m, in \u001b[0;36mbm4d_multichannel\u001b[0;34m(z, sigma_psd, profile, stage_arg)\u001b[0m\n\u001b[1;32m     81\u001b[0m full_denoi\u001b[38;5;241m.\u001b[39mappend(denoi)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(z)):\n\u001b[0;32m---> 84\u001b[0m     denoi \u001b[38;5;241m=\u001b[39m \u001b[43mbm4d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_sigma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage_arg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch_arrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     full_denoi\u001b[38;5;241m.\u001b[39mappend(denoi)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(z, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.12/site-packages/bm4d/__init__.py:217\u001b[0m, in \u001b[0;36mbm4d\u001b[0;34m(z, sigma_psd, profile, stage_arg, blockmatches)\u001b[0m\n\u001b[1;32m    213\u001b[0m t_forward, t_inverse, hadper_trans_single_den, \\\n\u001b[1;32m    214\u001b[0m     inverse_hadper_trans_single_den, wwin3d \u001b[38;5;241m=\u001b[39m _get_transforms(pro, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# Call the actual hard-thresholding step with the acquired parameters\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m y_hat, bm_out_ht \u001b[38;5;241m=\u001b[39m \u001b[43mht_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpsd_blur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpro\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqshifts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhadper_trans_single_den\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m                        \u001b[49m\u001b[43minverse_hadper_trans_single_den\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwwin3d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblockmatches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbm_in_ht\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# Residual denoising, HT\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pro\u001b[38;5;241m.\u001b[39mdenoise_residual:\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.12/site-packages/bm4d/bm4d_ctypes.py:606\u001b[0m, in \u001b[0;36mbm4d_ht\u001b[0;34m(z, psd, pro, t_forward, t_inverse, qshifts, hadper_trans_single_den, inverse_hadper_trans_single_den, wwin3d, refilter, blockmatches)\u001b[0m\n\u001b[1;32m    603\u001b[0m res \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(z_shape)\n\u001b[1;32m    605\u001b[0m c_z \u001b[38;5;241m=\u001b[39m conv_to_array(np\u001b[38;5;241m.\u001b[39mascontiguousarray(z\u001b[38;5;241m.\u001b[39mflatten(), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), ctype\u001b[38;5;241m=\u001b[39mctypes\u001b[38;5;241m.\u001b[39mc_float)\n\u001b[0;32m--> 606\u001b[0m c_psd \u001b[38;5;241m=\u001b[39m \u001b[43mconv_to_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpsd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_float\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m c_est \u001b[38;5;241m=\u001b[39m conv_to_array(np\u001b[38;5;241m.\u001b[39mascontiguousarray(res\u001b[38;5;241m.\u001b[39mflatten(), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), ctype\u001b[38;5;241m=\u001b[39mctypes\u001b[38;5;241m.\u001b[39mc_float)\n\u001b[1;32m    609\u001b[0m func_ht(c_z, c_psd, params, transforms, c_est,\n\u001b[1;32m    610\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mbyref(matchtables) \u001b[38;5;28;01mif\u001b[39;00m matchtables \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m matchtables)\n",
      "File \u001b[0;32m~/.conda/envs/DL/lib/python3.12/site-packages/bm4d/bm4d_ctypes.py:282\u001b[0m, in \u001b[0;36mconv_to_array\u001b[0;34m(pyarr, ctype)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconv_to_array\u001b[39m(pyarr, ctype\u001b[38;5;241m=\u001b[39mctypes\u001b[38;5;241m.\u001b[39mc_float):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    Convert Python array to C array\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03m    :param pyarr: python array\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m    :param ctype: type of resulting element\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    :return: C array (pointer)\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mctype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpyarr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpyarr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bs_wieners = [8,16]\n",
    "bs_hts = [8,16]\n",
    "process_bm3d(typ=\"Patch_Size\", bs_hts=bs_hts, bs_wieners=bs_wieners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Variable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
